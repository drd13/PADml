{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phys Astro Data: Deep Learning using Pytorch and Keras\n",
    "\n",
    "Objectives:\n",
    "* Explain the essential components behind training a neural network\n",
    "* Show simple practical example of how to program a neural network\n",
    "* Contrast the two leading frameworks (Pytorch and Keras) so that people can make an informed decision.\n",
    "* Try and make it useful for people who already know machine learning (with small links to papers and asides).\n",
    "\n",
    "We will only talk about neural networks (ie deep learning) as this is what these libraries are specialized for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Basics\n",
    "\n",
    "Machine learning = learning from examples.\n",
    "\n",
    "Let us begin with an example, consider we have a dataset containing images of digits as well as their **labels**, and we want to be able to predict the labels directly from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"https://corochann.com/wp-content/uploads/2017/02/mnist_plot-800x600.png\",width=200,height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want an automated process to design an algorithm, we need to be able to assess whether an algorithm is better than another. This is where the **loss function** comes in. A typical choice is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "   MSE = \\sum_{1}^{N} (y_i-\\hat{y}(x_i))^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Cross Entropy := -  \\sum_{1}^{N} \\sum_{c=1}^My_{o,c}\\log(p_{o,c})\n",
    "\\end{equation}\n",
    "where \n",
    "* M - number of classes (0,1,2,3,4,5,6,7,8,9 for the digit recognition)\n",
    "* y - binary indicator (0 or 1) if class label c is the correct classification for observation o\n",
    "* p - predicted probability observation o is of class c. \n",
    "\n",
    "This will be maximized when $p$ is exactly equal to the corresponding y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding an algorithm/function that can find a low value of the loss function. We need to have some flexible way of parametrizing this function. This is where the Neural Network comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*QVIyc5HnGDWTNX3m-nIm9w.png\",width=200,height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **neural network** contains many parameters in the form of edges and activation functions. The activations are defined as\n",
    "$a_j = \\Phi\\left ( \\sum_{i} w_{ij} a_i +b_j \\right )$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the neural network parameters, we change the mathematical function that the neural network is representing. With small changes to the parameters only leading to slight modifications in the function represented by the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look back to what we have done so far, through the neural network, we are able to generate many different algorithms by changing the parameters (weights, activations) and through the loss function we are able to assess which algorithm is the best. At this point, if we had infinite time we could just generate random weights and activations and keep the model that leads to the best loss function. This however, would take a really really really long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation** is what allows us to find good values for the edges and weights of our network. A full explanation is beyond the scope of this talk but we will explain intution behind it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is directly related to the **loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"https://blog.paperspace.com/content/images/2018/05/challenges-1.png\",width=10,height=10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation** is a way to get the gradient of the loss function, for the current set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the gradient of the loss function. We can make small changes to the parameters accordingly. If we iteratively repeat this procedure, we will then converge to a local minima. This is known as **gradient descent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*yasmQ5kvlmbYMe8eDkyl6w.png\",width=10,height=10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of illustration, we have only shown two dimensions. When we have many parameters, our loss function would have as many dimensions as there are parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize how a neural network is trained:  \n",
    "* Create a Neural network structure and initialize it with random parameter values (weights and activation functions). For our example we want as many inputs as image pixels and as many outputs as digit choices. \n",
    "\n",
    "* Define a loss function which measuring how well our neural network is doing at the task (how well does it predict digits). Before the neural network is trained, the neural network will be terrible at this task.\n",
    "\n",
    "* Iteratively, pass images through the neural network. Estimate the gradient of the parameters and update it. Iteratively improving the neural network output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters\n",
    "Hyperparameters are values/parameters set by the user before the learning begins, in a way similar to the input parameters for our physical models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_batch = 64\n",
    "n_batch_test = 1000\n",
    "n_classes = 10\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = torchvision.datasets.MNIST('.', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_mnist = torchvision.datasets.MNIST('.', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(train_mnist,batch_size=n_batch, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_mnist,batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at our mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_mnist[0][0][0].shape\n",
    "for i in range(3):\n",
    "    plt.title(\"label:{}\".format(train_mnist[i][1].numpy()))\n",
    "    plt.imshow(train_mnist[i][0][0], cmap='gray', interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, we define our neural network by inheriting from nn.module. The forward function is the function that is called when evaluating new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistNetwork, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(28*28, 1024),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Linear(1024, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512,256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(256,128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(128, 10)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "        #return F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MnistNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we next need to define a loss function, and choose which flavour of gradient descent to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "# create a loss function\n",
    "loss =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    for i,(data,label) in enumerate(loader):\n",
    "        #set the gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        #reshape data\n",
    "        data = data.view(n_batch,28*28)\n",
    "        #predict the labels for the inputs\n",
    "        pred_label = network(data)\n",
    "        #calculate the loss between the predicted and true labels\n",
    "        err =  loss(pred_label,label)\n",
    "        #backpropagate to get the derivatives\n",
    "        err.backward()\n",
    "        print(\"the loss is:{}\".format(err))\n",
    "        #update the weights in accordance with the backpropagation\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(data,label) in enumerate(test_loader):\n",
    "    data = data.view(n_batch_test,28*28)\n",
    "    pred = network(data)\n",
    "    pred_labels = torch.argmax(pred,1)\n",
    "    a = pred_labels == label\n",
    "    print(torch.sum(a).numpy()/n_batch_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high level API used to construct different neural networks. Users are allowed to use different deep learning libaraies as backend (The actual code that runs the model and computation), such as TensorFlow, CNTK, or Theano.  It is a more straight-forward and user friendly API compared to Tensorflow (without the sessions, placeholder etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "The dataset we are going to use today , MNIST, is one of the standard dataset commonly used for branchmarking, and thus Keras has created dedicated classes to import these standard datasets. \n",
    "There are other commonly used datasets, you can find them here:https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has made it into a class\n",
    "from keras.datasets import mnist\n",
    "## separate data into training set and test set.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets inspect the shape of each component:\n",
    "print(\"Training set:\",x_train.shape,\"Training Label:\",y_train.shape)\n",
    "print(\"Test set:\",x_test.shape,\"Test Label:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of each train and test set tells us how many 28x28 images are used for training and test, each images will have their own corresponding *labels*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise the data\n",
    "Neural Network works better when the data is normalised. The idea is to put every features (input data into a common scale). It is especially important when the input data has different ranges of values. There are two main reasons for this technique:\n",
    "1. Un-normalised data will cause the gradient to oscillate back and forth, and the neural network will end up taking longer time to complete the training\n",
    "The power of neural network relies on its ability to adjust weights, such that it can get closer to the optimum vlaue. The gradient computed at each iteration served as a guiding light for the Neural Network, so that it knows which direction it should move in order to get to the global max/min.\n",
    "2. Features with higher range (such as income as compared to age), will dominate the learning as it will have the highest variation when we change the weight. Normalising the features can put every features on the same ground and thus each of them will be considered equally.\n",
    "\n",
    "One of the most common normalisation techniques is to normalise the feature range to [0,1]. \n",
    "$$y = \\frac{x - x_{min}}{x_{max}-x_{min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
    "x_test = (x_test - x_test.min())/(x_test.max() - x_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the maximum and minimum:\n",
    "x_train.max(),x_train.min(),x_test.max(),x_test.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataformat for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 28*28)\n",
    "x_test = x_test.reshape(10000, 28*28)\n",
    "## turn them into one-hot vector\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "#further divides training data into training and validation data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0].reshape(28,28))\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr = learning_rate ),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train,batch_size=n_batch,\n",
    "                                     epochs=epochs,\n",
    "                                     verbose=1,\n",
    "                                     validation_data=(X_val, Y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "In the last section we have used Tensorboard to look at some simple computation graph. Keras provided a simple functionality to incorporate our model into Tensorboard. It can be used to monitor the training process, the entire architecture of the deep learning model and their associated weights, and many more: check out this link:\n",
    "https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise tensorboard call back object\n",
    "csvlogger = keras.callbacks.CSVLogger(\"./Graph/training_log.csv\", separator=',', append=False)\n",
    "\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', \n",
    "                                         histogram_freq=1, \n",
    "                                         write_graph=True, \n",
    "                                         write_images=False,update_freq='epoch',batch_size=n_batch)\n",
    "\n",
    "history = model.fit(X_train,Y_train,batch_size=n_batch,\n",
    "                                     epochs=epochs,\n",
    "                                     verbose=1,\n",
    "                                     validation_data=(X_val, Y_val), shuffle=True,callbacks=[tbCallBack,csvlogger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about Tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have been talking about PyTorch, Keras etc, but where is Tensorflow ?\n",
    "Tensorflow implementation is actually less straight forward. As you have seen in the previous seminar, to run a deep learning model we will need to the following:\n",
    "1. Define the variables (placeholders) for any input to the computational graph\n",
    "2. Define the computational graph ( in this case, our deep learning model)\n",
    "3. Define the loss function and optimizer\n",
    "4. Start a session and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "## Tensorflow has also created a module for MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data, set one hot to True, similar to what we did in keras.utils.to_categorical\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of nodes for different hidden layers\n",
    "n_nodes_hl1 = 1024\n",
    "n_nodes_hl2 = 512\n",
    "n_nodes_hl3 = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our variables. 784 is the flatten version of the 28x28 image\n",
    "input_data = tf.placeholder('float', [None, 28*28])\n",
    "target = tf.placeholder('float')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    #initialise weights for different hidden layers\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([28*28, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    layer1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "\n",
    "    layer2 = tf.add(tf.matmul(layer1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "\n",
    "    layer3 = tf.add(tf.matmul(layer2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "    output = tf.matmul(layer3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = neural_network_model(input_data)\n",
    "# OLD VERSION:\n",
    "#cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "# NEW:\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=target) )\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # OLD:\n",
    "    #sess.run(tf.initialize_all_variables())\n",
    "    # NEW:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ## epochs, number of times NN will see the data\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for _ in range(int(mnist.train.num_examples/n_batch)):\n",
    "            epoch_x, epoch_y = mnist.train.next_batch(n_batch)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={input_data: epoch_x, target: epoch_y})\n",
    "            epoch_loss += c\n",
    "\n",
    "        print('Epoch', epoch, 'completed out of',epochs,'loss:',epoch_loss)\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(target, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    print('Accuracy:',accuracy.eval({input_data:mnist.test.images, target:mnist.test.labels}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.test.labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras or Tensorflow? Why not both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2-Alpha\n",
    "\n",
    "Tensorflow 2 is a major update from the Tensforflow team. As quoted from the website, \"TensorFlow 2.0 focuses on simplicity and ease of use, with updates like eager execution, intuitive higher-level APIs, and flexible model building on any platform.\", lets have a look at the implementation from Tensorflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## import modules\n",
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "#!pip install -q tensorflow==2.0.0-alpha0\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "## load data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of using AI in the field of astronomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning of multi-element abundances from high-resolution spectroscopic data https://arxiv.org/abs/1808.04428\n",
    "\n",
    "\n",
    "Galaxy detection and identification using deep learning and dataaugmentation \n",
    "https://arxiv.org/pdf/1809.01691.pdf\n",
    "\n",
    "CosmoFlow: Using Deep Learning to Learn the Universe at Scale\n",
    "https://arxiv.org/pdf/1808.04728.pdf\n",
    "\n",
    "Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90\n",
    "https://iopscience.iop.org/article/10.3847/1538-3881/aa9e09/pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Neural Networks?\n",
    "\n",
    "Neural network are very flexible function estimators that learn through being shown \"examples\". Compared to other techniques, a strength of Neural Networks is that they can be customized to fit the structure of a dataset (Convolutional Neural Networks for images, Recurrent Neural Networks for time-series). Another advantage is that once trained, they can yield very quick predictions.\n",
    "\n",
    "Unlike other algorithms like Support Vector Machines or Linear Regression, the loss function they minimize is not convex (ie it does not have a single minimum). Instead it has multiple minimums, and in training we are interested in finding a suitably good minimum (as opposed to a global minimum). This means that their training can sometimes be a lot more fiddly. The loss function that you minimize can also be modified to fit the task you are trying to accomplish (see VAE or GAN for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually an interesting debate going on about whether deep learning methods should be assumption driven or whether they should be fully black-box methods. (see this open letter by max welling for example https://staff.fnwi.uva.nl/m.welling/wp-content/uploads/Model-versus-Data-AI-1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture shows the loss function with only 2 parameters. As always our intuition breaks down, neural networks will usually have tens of thousands of parameters. A minima will then only occur when all dimenions are at a minimum (otherwise saddle point). As such, its alot harder to reach a local minima. This is the same type of insight as the lottery ticket paper (see link). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
