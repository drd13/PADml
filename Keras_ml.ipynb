{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high level API used to construct different neural networks. Users are allowed to use different deep learning libaraies as backend (The actual code that runs the model and computation), such as TensorFlow, CNTK, or Theano.  It is a more straight-forward and user friendly API compared to Tensorflow (without the sessions, placeholder etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters\n",
    "Hyperparameters are values/parameters set by the user before the learning begins, in a way similar to the input parameters for our physical models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "The dataset we are going to use today , MNIST, is one of the standard dataset commonly used for branchmarking, and thus Keras has created dedicated classes to import these standard datasets. \n",
    "There are other commonly used datasets, you can find them here:https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has made it into a class\n",
    "from keras.datasets import mnist\n",
    "## separate data into training set and test set.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (60000, 28, 28) Training Label: (60000,)\n",
      "Test set: (10000, 28, 28) Test Label: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#lets inspect the shape of each component:\n",
    "print(\"Training set:\",x_train.shape,\"Training Label:\",y_train.shape)\n",
    "print(\"Test set:\",x_test.shape,\"Test Label:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of each train and test set tells us how many 28x28 images are used for training and test, each images will have their own corresponding *labels*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise the data\n",
    "Neural Network works better when the data is normalised. The idea is to put every features (input data into a common scale). It is especially important when the input data has different ranges of values. There are two main reasons for this technique:\n",
    "1. Un-normalised data will cause the gradient to oscillate back and forth, and the neural network will end up taking longer time to complete the training\n",
    "The power of neural network relies on its ability to adjust weights, such that it can get closer to the optimum vlaue. The gradient computed at each iteration served as a guiding light for the Neural Network, so that it knows which direction it should move in order to get to the global max/min.\n",
    "2. Features with higher range (such as income as compared to age), will dominate the learning as it will have the highest variation when we change the weight. Normalising the features can put every features on the same ground and thus each of them will be considered equally.\n",
    "\n",
    "One of the most common normalisation techniques is to normalise the feature range to [0,1]. \n",
    "$$y = \\frac{x - x_{min}}{x_{max}-x_{min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
    "x_test = (x_test - x_test.min())/(x_test.max() - x_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0, 1.0, 0.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the maximum and minimum:\n",
    "x_train.max(),x_train.min(),x_test.max(),x_test.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataformat for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "## turn them into one-hot vector\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "#further divides training data into training and validation data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape(28,28))\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gordonyip/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gordonyip/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/gordonyip/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 11s 201us/step - loss: 0.3514 - acc: 0.9048 - val_loss: 0.1367 - val_acc: 0.9642\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.1821 - acc: 0.9611 - val_loss: 0.1397 - val_acc: 0.9733\n",
      "Epoch 3/5\n",
      "54000/54000 [==============================] - 11s 203us/step - loss: 0.1630 - acc: 0.9669 - val_loss: 0.1514 - val_acc: 0.9742\n",
      "Epoch 4/5\n",
      "54000/54000 [==============================] - 12s 224us/step - loss: 0.1541 - acc: 0.9715 - val_loss: 0.2024 - val_acc: 0.9678\n",
      "Epoch 5/5\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 0.1614 - acc: 0.9731 - val_loss: 0.1995 - val_acc: 0.9703\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,Y_train,batch_size=batch_size,\n",
    "                                     epochs=epochs,\n",
    "                                     verbose=1,\n",
    "                                     validation_data=(X_val, Y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.21180870634457583\n",
      "Test accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section we have used Tensorboard to look at some simple computation graph. Keras provided a simple functionality to incorporate our model into Tensorboard. It can be used to monitor the training process, the entire architecture of the deep learning model and their associated weights, and many more: check out this link:\n",
    "https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.1557 - acc: 0.9816 - val_loss: 0.2170 - val_acc: 0.9795\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 8s 153us/step - loss: 0.1565 - acc: 0.9815 - val_loss: 0.2693 - val_acc: 0.9770\n",
      "Epoch 3/5\n",
      "54000/54000 [==============================] - 9s 158us/step - loss: 0.1635 - acc: 0.9830 - val_loss: 0.2691 - val_acc: 0.9767\n",
      "Epoch 4/5\n",
      "54000/54000 [==============================] - 9s 165us/step - loss: 0.1450 - acc: 0.9834 - val_loss: 0.2931 - val_acc: 0.9758\n",
      "Epoch 5/5\n",
      "54000/54000 [==============================] - 9s 158us/step - loss: 0.1481 - acc: 0.9832 - val_loss: 0.2775 - val_acc: 0.9743\n"
     ]
    }
   ],
   "source": [
    "## initialise tensorboard call back object\n",
    "csvlogger = keras.callbacks.CSVLogger(\"./Graph/training_log.csv\", separator=',', append=False)\n",
    "\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', \n",
    "                                         histogram_freq=1, \n",
    "                                         write_graph=True, \n",
    "                                         write_images=False,update_freq='epoch',batch_size=batch_size)\n",
    "\n",
    "history = model.fit(X_train,Y_train,batch_size=batch_size,\n",
    "                                     epochs=epochs,\n",
    "                                     verbose=1,\n",
    "                                     validation_data=(X_val, Y_val), shuffle=True,callbacks=[tbCallBack,csvlogger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about Tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have been talking about PyTorch, Keras etc, but where is Tensorflow ?\n",
    "Tensorflow implementation is actually less straight forward. As you have seen in the previous seminar, to run a deep learning model we will need to the following:\n",
    "1. Define the variables (placeholders) for any input to the computational graph\n",
    "2. Define the computational graph ( in this case, our deep learning model)\n",
    "3. Define the loss function and optimizer\n",
    "4. Start a session and train it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "## Tensorflow has also created a module for MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "## load the data, set one hot to True, similar to what we did in keras.utils.to_categorical\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of nodes for different hidden layers\n",
    "n_nodes_hl1 = 1024\n",
    "n_nodes_hl2 = 512\n",
    "n_nodes_hl3 = 128\n",
    "\n",
    "## Hyperparameters for training\n",
    "n_classes = 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our variables. 784 is the flatten version of the 28x28 image\n",
    "input_data = tf.placeholder('float', [None, 784])\n",
    "target = tf.placeholder('float')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    #initialise weights for different hidden layers\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    layer1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "\n",
    "    layer2 = tf.add(tf.matmul(layer1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "\n",
    "    layer3 = tf.add(tf.matmul(layer2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "    output = tf.matmul(layer3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 10 loss: 1680689.5903320312\n",
      "Epoch 1 completed out of 10 loss: 391256.8142089844\n",
      "Epoch 2 completed out of 10 loss: 217650.83019399643\n",
      "Epoch 3 completed out of 10 loss: 129485.53441876173\n",
      "Epoch 4 completed out of 10 loss: 79209.29220227778\n",
      "Epoch 5 completed out of 10 loss: 49299.8393509388\n",
      "Epoch 6 completed out of 10 loss: 32981.79262545705\n",
      "Epoch 7 completed out of 10 loss: 25062.074311968838\n",
      "Epoch 8 completed out of 10 loss: 21279.05865962361\n",
      "Epoch 9 completed out of 10 loss: 19255.72293722272\n",
      "Accuracy: 0.9516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = neural_network_model(input_data)\n",
    "# OLD VERSION:\n",
    "#cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "# NEW:\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=target) )\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "hm_epochs = 10\n",
    "with tf.Session() as sess:\n",
    "    # OLD:\n",
    "    #sess.run(tf.initialize_all_variables())\n",
    "    # NEW:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ## epochs, number of times NN will see the data\n",
    "    for epoch in range(hm_epochs):\n",
    "        epoch_loss = 0\n",
    "        for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "            epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={input_data: epoch_x, target: epoch_y})\n",
    "            epoch_loss += c\n",
    "\n",
    "        print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(target, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    print('Accuracy:',accuracy.eval({input_data:mnist.test.images, target:mnist.test.labels}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras or Tensorflow? Why not both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2-Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2 is a major update from the Tensforflow team. As quoted from the website, \"TensorFlow 2.0 focuses on simplicity and ease of use, with updates like eager execution, intuitive higher-level APIs, and flexible model building on any platform.\", lets have a look at the implementation from Tensorflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import modules\n",
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "#!pip install -q tensorflow==2.0.0-alpha0\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "## load data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of using AI in the field of astronomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://iopscience.iop.org/article/10.3847/1538-3881/aa9e09/pdf\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
